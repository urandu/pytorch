# WARNING: DO NOT EDIT THIS FILE DIRECTLY!!!
# See the README.md in this directory.

# IMPORTANT: To update Docker image version, please first update
# https://github.com/pytorch/ossci-job-dsl/blob/master/src/main/groovy/ossci/pytorch/DockerVersion.groovy and
# https://github.com/pytorch/ossci-job-dsl/blob/master/src/main/groovy/ossci/caffe2/DockerVersion.groovy,
# and then update DOCKER_IMAGE_VERSION at the top of the following files:
# * cimodel/data/pytorch_build_definitions.py
# * cimodel/data/caffe2_build_definitions.py
# And the inline copies of the variable in
# * verbatim-sources/job-specs-custom.yml
#   (grep for DOCKER_IMAGE)

version: 2.1

docker_config_defaults: &docker_config_defaults
  user: jenkins
  aws_auth:
    # This IAM user only allows read-write access to ECR
    aws_access_key_id: ${CIRCLECI_AWS_ACCESS_KEY_FOR_ECR_READ_WRITE_V4}
    aws_secret_access_key: ${CIRCLECI_AWS_SECRET_KEY_FOR_ECR_READ_WRITE_V4}

# This system setup script is meant to run before the CI-related scripts, e.g.,
# installing Git client, checking out code, setting up CI env, and
# building/testing.
setup_linux_system_environment: &setup_linux_system_environment
  name: Set Up System Environment
  no_output_timeout: "1h"
  command: ~/workspace/.circleci/scripts/setup_linux_system_environment.sh

# NB: This (and the command below) must be run after attaching
# ~/workspace.  This is NOT the default working directory (that's
# ~/project); this workspace is generated by the setup job.
should_run_job: &should_run_job
  name: Should Run Job After attach_workspace
  no_output_timeout: "2m"
  command: ~/workspace/.circleci/scripts/should_run_job.sh

setup_ci_environment: &setup_ci_environment
  name: Set Up CI Environment After attach_workspace
  no_output_timeout: "1h"
  command: ~/workspace/.circleci/scripts/setup_ci_environment.sh

# Installs expect and moreutils so that we can call `unbuffer` and `ts`.
# Also installs OpenMP
# !!!!NOTE!!!! this is copied into a binary_macos_brew_update job which is the
# same but does not install libomp. If you are changing this, consider if you
# need to change that step as well.
macos_brew_update: &macos_brew_update
  name: Brew update and install moreutils, expect and libomp
  no_output_timeout: "1h"
  command: |
    set -ex
    # See https://discourse.brew.sh/t/fetching-homebrew-repos-is-slow/5374/3
    brew untap caskroom/homebrew-cask
    # moreutils installs a `parallel` executable by default, which conflicts
    # with the executable from the GNU `parallel`, so we must unlink GNU
    # `parallel` first, and relink it afterwards
    brew update
    brew unlink parallel
    brew install moreutils
    brew link parallel --overwrite
    brew install expect
    brew install libomp


ios_brew_update: &ios_brew_update
  name: Install iOS toolchains
  no_output_timeout: "1h"
  command: |
    set -ex
    brew update
    brew unlink parallel
    brew install moreutils
    brew link parallel --overwrite
    brew install expect
    brew install libtool



##############################################################################
# Build parameters
##############################################################################
pytorch_params: &pytorch_params
  parameters:
    build_environment:
      type: string
      default: ""
    docker_image:
      type: string
      default: ""
    resource_class:
      type: string
      default: "large"
    use_cuda_docker_runtime:
      type: string
      default: ""
  environment:
    BUILD_ENVIRONMENT: << parameters.build_environment >>
    DOCKER_IMAGE: << parameters.docker_image >>
    USE_CUDA_DOCKER_RUNTIME: << parameters.use_cuda_docker_runtime >>
  resource_class: << parameters.resource_class >>

pytorch_ios_params: &pytorch_ios_params
  parameters:
    build_environment:
      type: string
      default: ""
    ios_arch:
      type: string
      default: ""
    ios_platform:
      type: string
      default: ""
    use_nnpack:
      type: string
      default: "ON"
  environment:
    BUILD_ENVIRONMENT: << parameters.build_environment >>
    IOS_ARCH: << parameters.ios_arch >>
    IOS_PLATFORM: << parameters.ios_platform >>
    USE_NNPACK: << parameters.use_nnpack >>
caffe2_params: &caffe2_params
  parameters:
    build_environment:
      type: string
      default: ""
    build_ios:
      type: string
      default: ""
    docker_image:
      type: string
      default: ""
    use_cuda_docker_runtime:
      type: string
      default: ""
    build_only:
      type: string
      default: ""
    resource_class:
      type: string
      default: "large"
  environment:
    BUILD_ENVIRONMENT: << parameters.build_environment >>
    BUILD_IOS: << parameters.build_ios >>
    USE_CUDA_DOCKER_RUNTIME: << parameters.use_cuda_docker_runtime >>
    DOCKER_IMAGE: << parameters.docker_image >>
    BUILD_ONLY: << parameters.build_only >>
  resource_class: << parameters.resource_class >>

##############################################################################
# Job specs
##############################################################################
jobs:
  pytorch_linux_build:
    <<: *pytorch_params
    machine:
      image: ubuntu-1604:201903-01
    steps:
    # See Note [Workspace for CircleCI scripts] in job-specs-setup.yml
    - attach_workspace:
        at: ~/workspace
    - run:
        <<: *should_run_job
    - run:
        <<: *setup_linux_system_environment
    - checkout
    - run:
        <<: *setup_ci_environment
    - run:
        name: Build
        no_output_timeout: "1h"
        command: |
          set -e
          # Pull Docker image and run build
          echo "DOCKER_IMAGE: "${DOCKER_IMAGE}
          docker pull ${DOCKER_IMAGE} >/dev/null
          export id=$(docker run -t -d -w /var/lib/jenkins ${DOCKER_IMAGE})

          git submodule sync && git submodule update -q --init --recursive

          docker cp /home/circleci/project/. $id:/var/lib/jenkins/workspace

          if [[ ${BUILD_ENVIRONMENT} == *"namedtensor"* ]]; then
            NAMED_FLAG="export BUILD_NAMEDTENSOR=1"
          fi

          # dispatch aten ops statically for mobile
          if [[ ${BUILD_ENVIRONMENT} == *"android"* ]]; then
            NAMED_FLAG="export USE_STATIC_DISPATCH=1"
          fi

          export COMMAND='((echo "export BUILD_ENVIRONMENT=${BUILD_ENVIRONMENT}" && echo '"$NAMED_FLAG"' && echo "source ./workspace/env" && echo "sudo chown -R jenkins workspace && cd workspace && .jenkins/pytorch/build.sh") | docker exec -u jenkins -i "$id" bash) 2>&1'
          echo ${COMMAND} > ./command.sh && unbuffer bash ./command.sh | ts

          # Push intermediate Docker image for next phase to use
          if [ -z "${BUILD_ONLY}" ]; then
            # Note [Special build images]
            # The namedtensor and xla builds use the same docker image as
            # pytorch-linux-trusty-py3.6-gcc5.4-build. In the push step, we have to
            # distinguish between them so the test can pick up the correct image.
            output_image=${DOCKER_IMAGE}-${CIRCLE_SHA1}
            if [[ ${BUILD_ENVIRONMENT} == *"namedtensor"* ]]; then
              export COMMIT_DOCKER_IMAGE=$output_image-namedtensor
            elif [[ ${BUILD_ENVIRONMENT} == *"xla"* ]]; then
              export COMMIT_DOCKER_IMAGE=$output_image-xla
            elif [[ ${BUILD_ENVIRONMENT} == *"android-ndk-r19c-x86_64"* ]]; then
              export COMMIT_DOCKER_IMAGE=$output_image-android-x86_64
            elif [[ ${BUILD_ENVIRONMENT} == *"android-ndk-r19c-arm-v7a"* ]]; then
              export COMMIT_DOCKER_IMAGE=$output_image-android-arm-v7a
            elif [[ ${BUILD_ENVIRONMENT} == *"android-ndk-r19c-arm-v8a"* ]]; then
              export COMMIT_DOCKER_IMAGE=$output_image-android-arm-v8a
            elif [[ ${BUILD_ENVIRONMENT} == *"android-ndk-r19c-x86_32"* ]]; then
              export COMMIT_DOCKER_IMAGE=$output_image-android-x86_32
            else
              export COMMIT_DOCKER_IMAGE=$output_image
            fi
            docker commit "$id" ${COMMIT_DOCKER_IMAGE}
            docker push ${COMMIT_DOCKER_IMAGE}
          fi

  pytorch_linux_test:
    <<: *pytorch_params
    machine:
      image: ubuntu-1604:201903-01
    steps:
    # See Note [Workspace for CircleCI scripts] in job-specs-setup.yml
    - attach_workspace:
        at: ~/workspace
    - run:
        <<: *should_run_job
    - run:
        <<: *setup_linux_system_environment
    - run:
        <<: *setup_ci_environment
    - run:
        name: Test
        no_output_timeout: "90m"
        command: |
          set -e
          # See Note [Special build images]
          output_image=${DOCKER_IMAGE}-${CIRCLE_SHA1}
          if [[ ${BUILD_ENVIRONMENT} == *"namedtensor"* ]]; then
            export COMMIT_DOCKER_IMAGE=$output_image-namedtensor
            NAMED_FLAG="export BUILD_NAMEDTENSOR=1"
          elif [[ ${BUILD_ENVIRONMENT} == *"xla"* ]]; then
            export COMMIT_DOCKER_IMAGE=$output_image-xla
          else
            export COMMIT_DOCKER_IMAGE=$output_image
          fi
          echo "DOCKER_IMAGE: "${COMMIT_DOCKER_IMAGE}
          docker pull ${COMMIT_DOCKER_IMAGE} >/dev/null
          if [ -n "${USE_CUDA_DOCKER_RUNTIME}" ]; then
            export id=$(docker run --runtime=nvidia -t -d -w /var/lib/jenkins ${COMMIT_DOCKER_IMAGE})
          else
            export id=$(docker run -t -d -w /var/lib/jenkins ${COMMIT_DOCKER_IMAGE})
          fi
          if [[ ${BUILD_ENVIRONMENT} == *"multigpu"* ]]; then
            export COMMAND='((echo "export BUILD_ENVIRONMENT=${BUILD_ENVIRONMENT}" && echo '"$NAMED_FLAG"' && echo "source ./workspace/env" && echo "sudo chown -R jenkins workspace && cd workspace && .jenkins/pytorch/multigpu-test.sh") | docker exec -u jenkins -i "$id" bash) 2>&1'
          else
            export COMMAND='((echo "export BUILD_ENVIRONMENT=${BUILD_ENVIRONMENT}" && echo '"$NAMED_FLAG"'&& echo "source ./workspace/env" && echo "sudo chown -R jenkins workspace && cd workspace && .jenkins/pytorch/test.sh") | docker exec -u jenkins -i "$id" bash) 2>&1'
          fi
          echo ${COMMAND} > ./command.sh && unbuffer bash ./command.sh | ts

  caffe2_linux_build:
    <<: *caffe2_params
    machine:
      image: ubuntu-1604:201903-01
    steps:
    # See Note [Workspace for CircleCI scripts] in job-specs-setup.yml
    - attach_workspace:
        at: ~/workspace
    - run:
        <<: *should_run_job
    - run:
        <<: *setup_linux_system_environment
    - checkout
    - run:
        <<: *setup_ci_environment
    - run:
        name: Build
        no_output_timeout: "1h"
        command: |
          set -e
          cat >/home/circleci/project/ci_build_script.sh \<<EOL
          # =================== The following code will be executed inside Docker container ===================
          set -ex
          export BUILD_ENVIRONMENT="$BUILD_ENVIRONMENT"

          # Reinitialize submodules
          git submodule sync && git submodule update -q --init --recursive

          # conda must be added to the path for Anaconda builds (this location must be
          # the same as that in install_anaconda.sh used to build the docker image)
          if [[ "${BUILD_ENVIRONMENT}" == conda* ]]; then
            export PATH=/opt/conda/bin:$PATH
            sudo chown -R jenkins:jenkins '/opt/conda'
          fi

          # Build
          ./.jenkins/caffe2/build.sh

          # Show sccache stats if it is running
          if pgrep sccache > /dev/null; then
            sccache --show-stats
          fi
          # =================== The above code will be executed inside Docker container ===================
          EOL
          chmod +x /home/circleci/project/ci_build_script.sh

          echo "DOCKER_IMAGE: "${DOCKER_IMAGE}
          docker pull ${DOCKER_IMAGE} >/dev/null
          export id=$(docker run -t -d -w /var/lib/jenkins ${DOCKER_IMAGE})
          docker cp /home/circleci/project/. $id:/var/lib/jenkins/workspace

          export COMMAND='((echo "source ./workspace/env" && echo "sudo chown -R jenkins workspace && cd workspace && ./ci_build_script.sh") | docker exec -u jenkins -i "$id" bash) 2>&1'
          echo ${COMMAND} > ./command.sh && unbuffer bash ./command.sh | ts

          # Push intermediate Docker image for next phase to use
          if [ -z "${BUILD_ONLY}" ]; then
            if [[ "$BUILD_ENVIRONMENT" == *cmake* ]]; then
              export COMMIT_DOCKER_IMAGE=${DOCKER_IMAGE}-cmake-${CIRCLE_SHA1}
            else
              export COMMIT_DOCKER_IMAGE=${DOCKER_IMAGE}-${CIRCLE_SHA1}
            fi
            docker commit "$id" ${COMMIT_DOCKER_IMAGE}
            docker push ${COMMIT_DOCKER_IMAGE}
          fi

  caffe2_linux_test:
    <<: *caffe2_params
    machine:
      image: ubuntu-1604:201903-01
    steps:
    # See Note [Workspace for CircleCI scripts] in job-specs-setup.yml
    - attach_workspace:
        at: ~/workspace
    - run:
        <<: *setup_linux_system_environment
    - run:
        <<: *should_run_job
    - run:
        <<: *setup_ci_environment
    - run:
        name: Test
        no_output_timeout: "1h"
        command: |
          set -e
          # TODO: merge this into Caffe2 test.sh
          cat >/home/circleci/project/ci_test_script.sh \<<EOL
          # =================== The following code will be executed inside Docker container ===================
          set -ex

          export BUILD_ENVIRONMENT="$BUILD_ENVIRONMENT"

          # libdc1394 (dependency of OpenCV) expects /dev/raw1394 to exist...
          sudo ln /dev/null /dev/raw1394

          # conda must be added to the path for Anaconda builds (this location must be
          # the same as that in install_anaconda.sh used to build the docker image)
          if [[ "${BUILD_ENVIRONMENT}" == conda* ]]; then
            export PATH=/opt/conda/bin:$PATH
          fi

          # Upgrade SSL module to avoid old SSL warnings
          pip -q install --user --upgrade pyOpenSSL ndg-httpsclient pyasn1

          pip -q install --user -b /tmp/pip_install_onnx "file:///var/lib/jenkins/workspace/third_party/onnx#egg=onnx"

          # Build
          ./.jenkins/caffe2/test.sh

          # Remove benign core dumps.
          # These are tests for signal handling (including SIGABRT).
          rm -f ./crash/core.fatal_signal_as.*
          rm -f ./crash/core.logging_test.*
          # =================== The above code will be executed inside Docker container ===================
          EOL
          chmod +x /home/circleci/project/ci_test_script.sh

          if [[ "$BUILD_ENVIRONMENT" == *cmake* ]]; then
            export COMMIT_DOCKER_IMAGE=${DOCKER_IMAGE}-cmake-${CIRCLE_SHA1}
          else
            export COMMIT_DOCKER_IMAGE=${DOCKER_IMAGE}-${CIRCLE_SHA1}
          fi
          echo "DOCKER_IMAGE: "${COMMIT_DOCKER_IMAGE}
          docker pull ${COMMIT_DOCKER_IMAGE} >/dev/null
          if [ -n "${USE_CUDA_DOCKER_RUNTIME}" ]; then
            export id=$(docker run --runtime=nvidia -t -d -w /var/lib/jenkins ${COMMIT_DOCKER_IMAGE})
          else
            export id=$(docker run -t -d -w /var/lib/jenkins ${COMMIT_DOCKER_IMAGE})
          fi
          docker cp /home/circleci/project/. "$id:/var/lib/jenkins/workspace"

          export COMMAND='((echo "source ./workspace/env" && echo "sudo chown -R jenkins workspace && cd workspace && ./ci_test_script.sh") | docker exec -u jenkins -i "$id" bash) 2>&1'
          echo ${COMMAND} > ./command.sh && unbuffer bash ./command.sh | ts

  caffe2_macos_build:
    <<: *caffe2_params
    macos:
      xcode: "9.0"
    steps:
      # See Note [Workspace for CircleCI scripts] in job-specs-setup.yml
      - attach_workspace:
          at: ~/workspace
      - run:
          <<: *should_run_job
      - checkout
      - run:
          <<: *macos_brew_update
      - run:
          name: Build
          no_output_timeout: "1h"
          command: |
            set -e

            export IN_CIRCLECI=1

            brew install cmake

            # Reinitialize submodules
            git submodule sync && git submodule update -q --init --recursive

            # Reinitialize path (see man page for path_helper(8))
            eval `/usr/libexec/path_helper -s`

            # Use Homebrew Python if configured to do so
            if [ "${PYTHON_INSTALLATION}" == "homebrew" ]; then
              export PATH=/usr/local/opt/python/libexec/bin:/usr/local/bin:$PATH
            fi

            pip -q install numpy

            # Install Anaconda if we need to
            if [ -n "${CAFFE2_USE_ANACONDA}" ]; then
              rm -rf ${TMPDIR}/anaconda
              curl -o ${TMPDIR}/conda.sh https://repo.continuum.io/miniconda/Miniconda${ANACONDA_VERSION}-latest-MacOSX-x86_64.sh
              chmod +x ${TMPDIR}/conda.sh
              /bin/bash ${TMPDIR}/conda.sh -b -p ${TMPDIR}/anaconda
              rm -f ${TMPDIR}/conda.sh
              export PATH="${TMPDIR}/anaconda/bin:${PATH}"
              source ${TMPDIR}/anaconda/bin/activate
            fi

            # Install sccache
            sudo curl https://s3.amazonaws.com/ossci-macos/sccache --output /usr/local/bin/sccache
            sudo chmod +x /usr/local/bin/sccache
            export SCCACHE_BUCKET=ossci-compiler-cache-circleci-v2

            # This IAM user allows write access to S3 bucket for sccache
            set +x
            export AWS_ACCESS_KEY_ID=${CIRCLECI_AWS_ACCESS_KEY_FOR_SCCACHE_S3_BUCKET_V4}
            export AWS_SECRET_ACCESS_KEY=${CIRCLECI_AWS_SECRET_KEY_FOR_SCCACHE_S3_BUCKET_V4}
            set -x

            export SCCACHE_BIN=${PWD}/sccache_bin
            mkdir -p ${SCCACHE_BIN}
            if which sccache > /dev/null; then
              printf "#!/bin/sh\nexec sccache $(which clang++) \$*" > "${SCCACHE_BIN}/clang++"
              chmod a+x "${SCCACHE_BIN}/clang++"

              printf "#!/bin/sh\nexec sccache $(which clang) \$*" > "${SCCACHE_BIN}/clang"
              chmod a+x "${SCCACHE_BIN}/clang"

              export PATH="${SCCACHE_BIN}:$PATH"
            fi

            # Build
            if [ "${BUILD_IOS:-0}" -eq 1 ]; then
              unbuffer scripts/build_ios.sh 2>&1 | ts
            elif [ -n "${CAFFE2_USE_ANACONDA}" ]; then
              # All conda build logic should be in scripts/build_anaconda.sh
              unbuffer scripts/build_anaconda.sh 2>&1 | ts
            else
              unbuffer scripts/build_local.sh 2>&1 | ts
            fi

            # Show sccache stats if it is running
            if which sccache > /dev/null; then
              sccache --show-stats
            fi

  
  setup:
    docker:
      - image: circleci/python:3.7.3
    steps:
      - checkout
      - run:
          name: Ensure config is up to date
          command: ./ensure-consistency.py
          working_directory: .circleci
      - run:
          name: Save commit message
          command: git log --format='%B' -n 1 HEAD > .circleci/scripts/COMMIT_MSG
      # Note [Workspace for CircleCI scripts]
      # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
      # In the beginning, you wrote your CI scripts in a
      # .circleci/config.yml file, and life was good.  Your CI
      # configurations flourished and multiplied.
      #
      # Then one day, CircleCI cometh down high and say, "Your YAML file
      # is too biggeth, it stresses our servers so."  And thus they
      # asketh us to smite the scripts in the yml file.
      #
      # But you can't just put the scripts in the .circleci folder,
      # because in some jobs, you don't ever actually checkout the
      # source repository.  Where you gonna get the scripts from?
      #
      # Here's how you do it: you persist .circleci/scripts into a
      # workspace, attach the workspace in your subjobs, and run all
      # your scripts from there.
      - persist_to_workspace:
          root: .
          paths: .circleci/scripts

  # pytorch_short_perf_test_gpu:
  #   environment:
  #     BUILD_ENVIRONMENT: pytorch-short-perf-test-gpu
  #     DOCKER_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-cuda9-cudnn7-py3:339"
  #     PYTHON_VERSION: "3.6"
  #     USE_CUDA_DOCKER_RUNTIME: "1"
  #   resource_class: gpu.medium
  #   machine:
  #     image: ubuntu-1604:201903-01
  #   steps:
  #   # See Note [Workspace for CircleCI scripts] in job-specs-setup.yml
  #   - attach_workspace:
  #       at: ~/workspace
  #   - run:
  #       <<: *should_run_job
  #   - run:
  #       <<: *setup_linux_system_environment
  #   - run:
  #       <<: *setup_ci_environment
  #   - run:
  #       name: Perf Test
  #       no_output_timeout: "1h"
  #       command: |
  #         set -e
  #         export COMMIT_DOCKER_IMAGE=${DOCKER_IMAGE}-${CIRCLE_SHA1}
  #         echo "DOCKER_IMAGE: "${COMMIT_DOCKER_IMAGE}
  #         docker pull ${COMMIT_DOCKER_IMAGE} >/dev/null
  #         export id=$(docker run --runtime=nvidia -t -d -w /var/lib/jenkins ${COMMIT_DOCKER_IMAGE})

  #         docker cp $id:/var/lib/jenkins/workspace/env /home/circleci/project/env
  #         # This IAM user allows write access to S3 bucket for perf test numbers
  #         set +x
  #         echo "declare -x AWS_ACCESS_KEY_ID=${CIRCLECI_AWS_ACCESS_KEY_FOR_PERF_TEST_S3_BUCKET_V4}" >> /home/circleci/project/env
  #         echo "declare -x AWS_SECRET_ACCESS_KEY=${CIRCLECI_AWS_SECRET_KEY_FOR_PERF_TEST_S3_BUCKET_V4}" >> /home/circleci/project/env
  #         set -x
  #         docker cp /home/circleci/project/env $id:/var/lib/jenkins/workspace/env

  #         export COMMAND='((echo "export BUILD_ENVIRONMENT=${BUILD_ENVIRONMENT}" && echo "source ./workspace/env" && echo "sudo chown -R jenkins workspace && cd workspace && .jenkins/pytorch/short-perf-test-gpu.sh") | docker exec -u jenkins -i "$id" bash) 2>&1'
  #         echo ${COMMAND} > ./command.sh && unbuffer bash ./command.sh | ts

  # pytorch_python_doc_push:
  #   environment:
  #     BUILD_ENVIRONMENT: pytorch-python-doc-push
  #     # TODO: stop hardcoding this
  #     DOCKER_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-cuda9-cudnn7-py3:339"
  #   resource_class: large
  #   machine:
  #     image: ubuntu-1604:201903-01
  #   steps:
  #   # See Note [Workspace for CircleCI scripts] in job-specs-setup.yml
  #   - attach_workspace:
  #       at: ~/workspace
  #   - run:
  #       <<: *should_run_job
  #   - run:
  #       <<: *setup_linux_system_environment
  #   - run:
  #       <<: *setup_ci_environment
  #   - run:
  #       name: Doc Build and Push
  #       no_output_timeout: "1h"
  #       command: |
  #         set -ex
  #         export COMMIT_DOCKER_IMAGE=${DOCKER_IMAGE}-${CIRCLE_SHA1}
  #         echo "DOCKER_IMAGE: "${COMMIT_DOCKER_IMAGE}
  #         docker pull ${COMMIT_DOCKER_IMAGE} >/dev/null
  #         export id=$(docker run -t -d -w /var/lib/jenkins ${COMMIT_DOCKER_IMAGE})

  #         # master branch docs push
  #         if [[ "${CIRCLE_BRANCH}" == "master" ]]; then
  #           export COMMAND='((echo "export BUILD_ENVIRONMENT=${BUILD_ENVIRONMENT}" && echo "export GITHUB_PYTORCHBOT_TOKEN=${GITHUB_PYTORCHBOT_TOKEN}" && echo "source ./workspace/env" && echo "sudo chown -R jenkins workspace && cd workspace && . ./.circleci/scripts/python_doc_push_script.sh docs/master master site") | docker exec -u jenkins -i "$id" bash) 2>&1'

  #         # stable release docs push. Due to some circleci limitations, we keep
  #         # an eternal PR open for merging v1.2.0 -> master for this job.
  #         # XXX: The following code is only run on the v1.2.0 branch, which might
  #         # not be exactly the same as what you see here.
  #         elif [[ "${CIRCLE_BRANCH}" == "v1.2.0" ]]; then
  #           export COMMAND='((echo "export BUILD_ENVIRONMENT=${BUILD_ENVIRONMENT}" && echo "export GITHUB_PYTORCHBOT_TOKEN=${GITHUB_PYTORCHBOT_TOKEN}" && echo "source ./workspace/env" && echo "sudo chown -R jenkins workspace && cd workspace && . ./.circleci/scripts/python_doc_push_script.sh docs/stable 1.2.0 site dry_run") | docker exec -u jenkins -i "$id" bash) 2>&1'

  #         # For open PRs: Do a dry_run of the docs build, don't push build
  #         else
  #           export COMMAND='((echo "export BUILD_ENVIRONMENT=${BUILD_ENVIRONMENT}" && echo "export GITHUB_PYTORCHBOT_TOKEN=${GITHUB_PYTORCHBOT_TOKEN}" && echo "source ./workspace/env" && echo "sudo chown -R jenkins workspace && cd workspace && . ./.circleci/scripts/python_doc_push_script.sh docs/master master site dry_run") | docker exec -u jenkins -i "$id" bash) 2>&1'
  #         fi

  #         echo ${COMMAND} > ./command.sh && unbuffer bash ./command.sh | ts

  #         # Save the docs build so we can debug any problems
  #         export DEBUG_COMMIT_DOCKER_IMAGE=${COMMIT_DOCKER_IMAGE}-debug
  #         docker commit "$id" ${DEBUG_COMMIT_DOCKER_IMAGE}
  #         docker push ${DEBUG_COMMIT_DOCKER_IMAGE}

  # pytorch_cpp_doc_push:
  #   environment:
  #     BUILD_ENVIRONMENT: pytorch-cpp-doc-push
  #     DOCKER_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-cuda9-cudnn7-py3:339"
  #   resource_class: large
  #   machine:
  #     image: ubuntu-1604:201903-01
  #   steps:
  #   # See Note [Workspace for CircleCI scripts] in job-specs-setup.yml
  #   - attach_workspace:
  #       at: ~/workspace
  #   - run:
  #       <<: *should_run_job
  #   - run:
  #       <<: *setup_linux_system_environment
  #   - run:
  #       <<: *setup_ci_environment
  #   - run:
  #       name: Doc Build and Push
  #       no_output_timeout: "1h"
  #       command: |
  #         set -ex
  #         export COMMIT_DOCKER_IMAGE=${DOCKER_IMAGE}-${CIRCLE_SHA1}
  #         echo "DOCKER_IMAGE: "${COMMIT_DOCKER_IMAGE}
  #         docker pull ${COMMIT_DOCKER_IMAGE} >/dev/null
  #         export id=$(docker run -t -d -w /var/lib/jenkins ${COMMIT_DOCKER_IMAGE})

  #         # master branch docs push
  #         if [[ "${CIRCLE_BRANCH}" == "master" ]]; then
  #           export COMMAND='((echo "export BUILD_ENVIRONMENT=${BUILD_ENVIRONMENT}" && echo "export GITHUB_PYTORCHBOT_TOKEN=${GITHUB_PYTORCHBOT_TOKEN}" && echo "source ./workspace/env" && echo "sudo chown -R jenkins workspace && cd workspace && . ./.circleci/scripts/cpp_doc_push_script.sh docs/master master") | docker exec -u jenkins -i "$id" bash) 2>&1'

  #         # stable release docs push. Due to some circleci limitations, we keep
  #         # an eternal PR open (#16502) for merging v1.0.1 -> master for this job.
  #         # XXX: The following code is only run on the v1.0.1 branch, which might
  #         # not be exactly the same as what you see here.
  #         elif [[ "${CIRCLE_BRANCH}" == "v1.0.1" ]]; then
  #           export COMMAND='((echo "export BUILD_ENVIRONMENT=${BUILD_ENVIRONMENT}" && echo "export GITHUB_PYTORCHBOT_TOKEN=${GITHUB_PYTORCHBOT_TOKEN}" && echo "source ./workspace/env" && echo "sudo chown -R jenkins workspace && cd workspace && . ./.circleci/scripts/cpp_doc_push_script.sh docs/stable 1.0.1") | docker exec -u jenkins -i "$id" bash) 2>&1'

  #         # For open PRs: Do a dry_run of the docs build, don't push build
  #         else
  #           export COMMAND='((echo "export BUILD_ENVIRONMENT=${BUILD_ENVIRONMENT}" && echo "export GITHUB_PYTORCHBOT_TOKEN=${GITHUB_PYTORCHBOT_TOKEN}" && echo "source ./workspace/env" && echo "sudo chown -R jenkins workspace && cd workspace && . ./.circleci/scripts/cpp_doc_push_script.sh docs/master master dry_run") | docker exec -u jenkins -i "$id" bash) 2>&1'
  #         fi

  #         echo ${COMMAND} > ./command.sh && unbuffer bash ./command.sh | ts

  #         # Save the docs build so we can debug any problems
  #         export DEBUG_COMMIT_DOCKER_IMAGE=${COMMIT_DOCKER_IMAGE}-debug
  #         docker commit "$id" ${DEBUG_COMMIT_DOCKER_IMAGE}
  #         docker push ${DEBUG_COMMIT_DOCKER_IMAGE}

  # pytorch_macos_10_13_py3_build:
  #   environment:
  #     BUILD_ENVIRONMENT: pytorch-macos-10.13-py3-build
  #   macos:
  #     xcode: "9.0"
  #   steps:
  #     # See Note [Workspace for CircleCI scripts] in job-specs-setup.yml
  #     - attach_workspace:
  #         at: ~/workspace
  #     - run:
  #         <<: *should_run_job
  #     - checkout
  #     - run:
  #         <<: *macos_brew_update
  #     - run:
  #         name: Build
  #         no_output_timeout: "1h"
  #         command: |
  #           set -e

  #           export IN_CIRCLECI=1

  #           # Install sccache
  #           sudo curl https://s3.amazonaws.com/ossci-macos/sccache --output /usr/local/bin/sccache
  #           sudo chmod +x /usr/local/bin/sccache

  #           export SCCACHE_BUCKET=ossci-compiler-cache-circleci-v2
  #           # This IAM user allows write access to S3 bucket for sccache
  #           set +x
  #           export AWS_ACCESS_KEY_ID=${CIRCLECI_AWS_ACCESS_KEY_FOR_SCCACHE_S3_BUCKET_V4}
  #           export AWS_SECRET_ACCESS_KEY=${CIRCLECI_AWS_SECRET_KEY_FOR_SCCACHE_S3_BUCKET_V4}
  #           set -x

  #           chmod a+x .jenkins/pytorch/macos-build.sh
  #           unbuffer .jenkins/pytorch/macos-build.sh 2>&1 | ts

  #           mkdir -p /Users/distiller/pytorch-ci-env/workspace

  #           # copy with -a to preserve relative structure (e.g., symlinks), and be recursive
  #           cp -a /Users/distiller/project/. /Users/distiller/pytorch-ci-env/workspace
  #     - persist_to_workspace:
  #         root: /Users/distiller/pytorch-ci-env
  #         paths:
  #           - "*"

  # pytorch_macos_10_13_py3_test:
  #   environment:
  #     BUILD_ENVIRONMENT: pytorch-macos-10.13-py3-test
  #   macos:
  #     xcode: "9.0"
  #   steps:
  #     # See Note [Workspace for CircleCI scripts] in job-specs-setup.yml
  #     # This workspace also carries binaries from the build job
  #     - attach_workspace:
  #         at: ~/workspace
  #     - run:
  #         <<: *should_run_job
  #     - run:
  #         <<: *macos_brew_update
  #     - run:
  #         name: Test
  #         no_output_timeout: "1h"
  #         command: |
  #           set -e
  #           export IN_CIRCLECI=1

  #           # copy with -a to preserve relative structure (e.g., symlinks), and be recursive
  #           # TODO: I'm not sure why we can't just run our job in
  #           # ~/workspace and call it a day
  #           # NB: Yes, you need workspace twice
  #           cp -a ~/workspace/workspace/. /Users/distiller/project

  #           chmod a+x .jenkins/pytorch/macos-test.sh
  #           unbuffer .jenkins/pytorch/macos-test.sh 2>&1 | ts

  # pytorch_macos_10_13_cuda9_2_cudnn7_py3_build:
  #   environment:
  #     BUILD_ENVIRONMENT: pytorch-macos-10.13-cuda9.2-cudnn7-py3-build
  #   macos:
  #     xcode: "9.0"
  #   steps:
  #     # See Note [Workspace for CircleCI scripts] in job-specs-setup.yml
  #     - attach_workspace:
  #         at: ~/workspace
  #     - run:
  #         <<: *should_run_job
  #     - checkout
  #     - run:
  #         <<: *macos_brew_update
  #     - run:
  #         name: Build
  #         no_output_timeout: "1h"
  #         command: |
  #           set -e

  #           export IN_CIRCLECI=1

  #           # Install CUDA 9.2
  #           sudo rm -rf ~/cuda_9.2.64_mac_installer.app || true
  #           curl https://s3.amazonaws.com/ossci-macos/cuda_9.2.64_mac_installer.zip -o ~/cuda_9.2.64_mac_installer.zip
  #           unzip ~/cuda_9.2.64_mac_installer.zip -d ~/
  #           sudo ~/cuda_9.2.64_mac_installer.app/Contents/MacOS/CUDAMacOSXInstaller --accept-eula --no-window
  #           sudo cp /usr/local/cuda/lib/libcuda.dylib /Developer/NVIDIA/CUDA-9.2/lib/libcuda.dylib
  #           sudo rm -rf /usr/local/cuda || true

  #           # Install cuDNN 7.1 for CUDA 9.2
  #           curl https://s3.amazonaws.com/ossci-macos/cudnn-9.2-osx-x64-v7.1.tgz -o ~/cudnn-9.2-osx-x64-v7.1.tgz
  #           rm -rf ~/cudnn-9.2-osx-x64-v7.1 && mkdir ~/cudnn-9.2-osx-x64-v7.1
  #           tar -xzvf ~/cudnn-9.2-osx-x64-v7.1.tgz -C ~/cudnn-9.2-osx-x64-v7.1
  #           sudo cp ~/cudnn-9.2-osx-x64-v7.1/cuda/include/cudnn.h /Developer/NVIDIA/CUDA-9.2/include/
  #           sudo cp ~/cudnn-9.2-osx-x64-v7.1/cuda/lib/libcudnn* /Developer/NVIDIA/CUDA-9.2/lib/
  #           sudo chmod a+r /Developer/NVIDIA/CUDA-9.2/include/cudnn.h /Developer/NVIDIA/CUDA-9.2/lib/libcudnn*

  #           # Install sccache
  #           sudo curl https://s3.amazonaws.com/ossci-macos/sccache --output /usr/local/bin/sccache
  #           sudo chmod +x /usr/local/bin/sccache
  #           export SCCACHE_BUCKET=ossci-compiler-cache-circleci-v2
  #           # This IAM user allows write access to S3 bucket for sccache
  #           set +x
  #           export AWS_ACCESS_KEY_ID=${CIRCLECI_AWS_ACCESS_KEY_FOR_SCCACHE_S3_BUCKET_V4}
  #           export AWS_SECRET_ACCESS_KEY=${CIRCLECI_AWS_SECRET_KEY_FOR_SCCACHE_S3_BUCKET_V4}
  #           set -x

  #           git submodule sync && git submodule update -q --init --recursive
  #           chmod a+x .jenkins/pytorch/macos-build.sh
  #           unbuffer .jenkins/pytorch/macos-build.sh 2>&1 | ts

  # pytorch_android_gradle_build:
  #   environment:
  #     BUILD_ENVIRONMENT: pytorch-linux-xenial-py3-clang5-android-ndk-r19c-gradle-build
  #     DOCKER_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-py3-clang5-android-ndk-r19c:339"
  #     PYTHON_VERSION: "3.6"
  #   resource_class: large
  #   machine:
  #     image: ubuntu-1604:201903-01
  #   steps:
  #   - attach_workspace:
  #       at: ~/workspace
  #   - run:
  #       <<: *should_run_job
  #   - run:
  #       <<: *setup_linux_system_environment
  #   - checkout
  #   - run:
  #       <<: *setup_ci_environment
  #   - run:
  #       name: pytorch android gradle build
  #       no_output_timeout: "1h"
  #       command: |
  #         set -eux
  #         docker_image_commit=${DOCKER_IMAGE}-${CIRCLE_SHA1}

  #         docker_image_libtorch_android_x86_32=${docker_image_commit}-android-x86_32
  #         docker_image_libtorch_android_x86_64=${docker_image_commit}-android-x86_64
  #         docker_image_libtorch_android_arm_v7a=${docker_image_commit}-android-arm-v7a
  #         docker_image_libtorch_android_arm_v8a=${docker_image_commit}-android-arm-v8a

  #         echo "docker_image_commit: "${docker_image_commit}
  #         echo "docker_image_libtorch_android_x86_32: "${docker_image_libtorch_android_x86_32}
  #         echo "docker_image_libtorch_android_x86_64: "${docker_image_libtorch_android_x86_64}
  #         echo "docker_image_libtorch_android_arm_v7a: "${docker_image_libtorch_android_arm_v7a}
  #         echo "docker_image_libtorch_android_arm_v8a: "${docker_image_libtorch_android_arm_v8a}

  #         # x86_32
  #         docker pull ${docker_image_libtorch_android_x86_32} >/dev/null
  #         export id_x86_32=$(docker run -t -d -w /var/lib/jenkins ${docker_image_libtorch_android_x86_32})

  #         export COMMAND='((echo "source ./workspace/env" && echo "sudo chown -R jenkins workspace") | docker exec -u jenkins -i "$id_x86_32" bash) 2>&1'
  #         echo ${COMMAND} > ./command.sh && unbuffer bash ./command.sh | ts

  #         # arm-v7a
  #         docker pull ${docker_image_libtorch_android_arm_v7a} >/dev/null
  #         export id_arm_v7a=$(docker run -t -d -w /var/lib/jenkins ${docker_image_libtorch_android_arm_v7a})

  #         export COMMAND='((echo "source ./workspace/env" && echo "sudo chown -R jenkins workspace") | docker exec -u jenkins -i "$id_arm_v7a" bash) 2>&1'
  #         echo ${COMMAND} > ./command.sh && unbuffer bash ./command.sh | ts

  #         mkdir ~/workspace/build_android_install_arm_v7a
  #         docker cp $id_arm_v7a:/var/lib/jenkins/workspace/build_android/install ~/workspace/build_android_install_arm_v7a

  #         # x86_64
  #         docker pull ${docker_image_libtorch_android_x86_64} >/dev/null
  #         export id_x86_64=$(docker run -t -d -w /var/lib/jenkins ${docker_image_libtorch_android_x86_64})

  #         export COMMAND='((echo "source ./workspace/env" && echo "sudo chown -R jenkins workspace") | docker exec -u jenkins -i "$id_x86_64" bash) 2>&1'
  #         echo ${COMMAND} > ./command.sh && unbuffer bash ./command.sh | ts

  #         mkdir ~/workspace/build_android_install_x86_64
  #         docker cp $id_x86_64:/var/lib/jenkins/workspace/build_android/install ~/workspace/build_android_install_x86_64

  #         # arm-v8a
  #         docker pull ${docker_image_libtorch_android_arm_v8a} >/dev/null
  #         export id_arm_v8a=$(docker run -t -d -w /var/lib/jenkins ${docker_image_libtorch_android_arm_v8a})

  #         export COMMAND='((echo "source ./workspace/env" && echo "sudo chown -R jenkins workspace") | docker exec -u jenkins -i "$id_arm_v8a" bash) 2>&1'
  #         echo ${COMMAND} > ./command.sh && unbuffer bash ./command.sh | ts

  #         mkdir ~/workspace/build_android_install_arm_v8a
  #         docker cp $id_arm_v8a:/var/lib/jenkins/workspace/build_android/install ~/workspace/build_android_install_arm_v8a

  #         docker cp ~/workspace/build_android_install_arm_v7a $id_x86_32:/var/lib/jenkins/workspace/build_android_install_arm_v7a
  #         docker cp ~/workspace/build_android_install_x86_64 $id_x86_32:/var/lib/jenkins/workspace/build_android_install_x86_64
  #         docker cp ~/workspace/build_android_install_arm_v8a $id_x86_32:/var/lib/jenkins/workspace/build_android_install_arm_v8a

  #         # run gradle buildRelease
  #         export COMMAND='((echo "source ./workspace/env" && echo "export BUILD_ENVIRONMENT=${BUILD_ENVIRONMENT}" && echo "sudo chown -R jenkins workspace && cd workspace && ./.circleci/scripts/build_android_gradle.sh") | docker exec -u jenkins -i "$id_x86_32" bash) 2>&1'
  #         echo ${COMMAND} > ./command.sh && unbuffer bash ./command.sh | ts

  #         mkdir -p ~/workspace/build_android_artifacts
  #         docker cp $id_x86_32:/var/lib/jenkins/workspace/android/artifacts.tgz ~/workspace/build_android_artifacts/

  #         output_image=$docker_image_libtorch_android_x86_32-gradle
  #         docker commit "$id_x86_32" ${output_image}
  #         docker push ${output_image}
  #   - store_artifacts:
  #       path: ~/workspace/build_android_artifacts/artifacts.tgz
  #       destination: artifacts.tgz

  # pytorch_android_gradle_build-x86_32:
  #   environment:
  #     BUILD_ENVIRONMENT: pytorch-linux-xenial-py3-clang5-android-ndk-r19c-gradle-build-only-x86_32
  #     DOCKER_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-py3-clang5-android-ndk-r19c:339"
  #     PYTHON_VERSION: "3.6"
  #   resource_class: large
  #   machine:
  #     image: ubuntu-1604:201903-01
  #   steps:
  #   - attach_workspace:
  #       at: ~/workspace
  #   - run:
  #       <<: *should_run_job
  #   - run:
  #       name: filter out not PR runs
  #       no_output_timeout: "5m"
  #       command: |
  #         echo "CIRCLE_PULL_REQUEST: ${CIRCLE_PULL_REQUEST:-}"
  #         if [ -z "${CIRCLE_PULL_REQUEST:-}" ]; then
  #           circleci step halt
  #         fi
  #   - run:
  #       <<: *setup_linux_system_environment
  #   - checkout
  #   - run:
  #       <<: *setup_ci_environment
  #   - run:
  #       name: pytorch android gradle build only x86_32 (for PR)
  #       no_output_timeout: "1h"
  #       command: |
  #         set -e
  #         docker_image_libtorch_android_x86_32=${DOCKER_IMAGE}-${CIRCLE_SHA1}-android-x86_32
  #         echo "docker_image_libtorch_android_x86_32: "${docker_image_libtorch_android_x86_32}

  #         # x86
  #         docker pull ${docker_image_libtorch_android_x86_32} >/dev/null
  #         export id=$(docker run -t -d -w /var/lib/jenkins ${docker_image_libtorch_android_x86_32})

  #         export COMMAND='((echo "source ./workspace/env" && echo "export BUILD_ENVIRONMENT=${BUILD_ENVIRONMENT}" && echo "sudo chown -R jenkins workspace && cd workspace && ./.circleci/scripts/build_android_gradle.sh") | docker exec -u jenkins -i "$id" bash) 2>&1'
  #         echo ${COMMAND} > ./command.sh && unbuffer bash ./command.sh | ts

  #         mkdir -p ~/workspace/build_android_x86_32_artifacts
  #         docker cp $id:/var/lib/jenkins/workspace/android/artifacts.tgz ~/workspace/build_android_x86_32_artifacts/

  #         output_image=${DOCKER_IMAGE}-${CIRCLE_SHA1}-android-gradle-x86_32
  #         docker commit "$id" ${output_image}
  #         docker push ${output_image}
  #   - store_artifacts:
  #       path: ~/workspace/build_android_x86_32_artifacts/artifacts.tgz
  #       destination: artifacts.tgz
  
  pytorch_ios_build:
    <<: *pytorch_ios_params  
    macos:
      xcode: "10.2.1"
    steps:
      # See Note [Workspace for CircleCI scripts] in job-specs-setup.yml
      - attach_workspace:
          at: ~/workspace
      - run:
          <<: *should_run_job
      - checkout
      - run:
          <<: *ios_brew_update
      - run:
          name: Build
          no_output_timeout: "1h"
          contxt: org-member
          command: |
            set -e
            # export IN_CIRCLECI=1
            WORKSPACE=/Users/distiller/workspace
            PROJ_ROOT=/Users/distiller/project
            export TCLLIBPATH="/usr/local/lib" 
            
            # Install conda
            curl -o ~/Downloads/conda.sh https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-x86_64.sh
            chmod +x ~/Downloads/conda.sh
            /bin/bash ~/Downloads/conda.sh -b -p ~/anaconda
            export PATH="~/anaconda/bin:${PATH}"
            source ~/anaconda/bin/activate
            # Install dependencies
            conda install numpy ninja pyyaml mkl mkl-include setuptools cmake cffi typing requests
          
            # sync submodules
            cd ${PROJ_ROOT}
            git submodule sync
            git submodule update --init --recursive

            # export 
            export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-"$(dirname $(which conda))/../"}
            
            # run build script
            chmod a+x ${PROJ_ROOT}/scripts/build_ios.sh
            echo "########################################################"
            cat ${PROJ_ROOT}/scripts/build_ios.sh
            echo "########################################################"

            echo "IOS_ARCH: ${IOS_ARCH}"
            echo "IOS_PLATFORM: ${IOS_PLATFORM}"
            BUILD_PYTORCH_MOBILE=1
            # unbuffer ${PROJ_ROOT}/scripts/build_ios.sh 2>&1 | ts

            CMAKE_ARGS=() 
            if [ -n "${IOS_ARCH:-}" ]; then
              CMAKE_ARGS+=("-DIOS_ARCH=${IOS_ARCH}")
            fi
            if [ -n "${IOS_PLATFORM:-}" ]; then
              CMAKE_ARGS+=("-DIOS_PLATFORM=${IOS_PLATFORM}")
            fi
            if [ -n "${USE_NNPACK:-}" ]; then 
              CMAKE_ARGS+=("-DUSE_NNPACK=${USE_NNPACK}")
            fi 
            CMAKE_ARGS+=("-DBUILD_CAFFE2_MOBILE=OFF")
            CMAKE_ARGS+=("-DCMAKE_PREFIX_PATH=$(python -c 'from distutils.sysconfig import get_python_lib; print(get_python_lib())')")
            CMAKE_ARGS+=("-DPYTHON_EXECUTABLE=$(python -c 'import sys; print(sys.executable)')")

            unbuffer ${PROJ_ROOT}/scripts/build_ios.sh ${CMAKE_ARGS[@]} 2>&1 | ts

            #archive the static libraries
            cd ${PROJ_ROOT}/build_ios/install/lib
                        
            ####### for testing ########
            rm -rf ${PROJ_ROOT}/build_ios/install/lib/libprotobuf-lite.a
            rm -rf ${PROJ_ROOT}/build_ios/install/lib/libpthreadpool.a
            
            #############################
            OUTPUT=libtorch_${IOS_ARCH}.a
            cd ${PROJ_ROOT}/build_ios/install/lib
            libtool -static *.a -o ${OUTPUT}
            
            #store the binary
            mkdir -p ${WORKSPACE}/zip/install
            cp -R ${PROJ_ROOT}/build_ios/install/include ${WORKSPACE}/zip/install
            mkdir -p ${WORKSPACE}/zip/install/lib
            cp ${PROJ_ROOT}/build_ios/install/lib/${OUTPUT}  ${WORKSPACE}/zip/install/lib

            #copy files
            mkdir -p ${WORKSPACE}/zip/src
            touch ${WORKSPACE}/zip/src/LibTorch.h
            echo "import <torch/script.h>" > ${WORKSPACE}/zip/src/LibTorch.h
            # cp ${PROJ_ROOT}/ios/LibTorch.h ${WORKSPACE}/zip/src
            # cp ${PROJ_ROOT}/ios/framework.module ${WORKSPACE}/zip/src

            # zip the library
            cd ${WORKSPACE}/zip/
            export FILE_NAME=libtorch_ios_`date +%m%d%Y`_${IOS_ARCH}.zip
            zip -r ${FILE_NAME} install src
            
            # upload to aws
            brew install awscli
            set +x
            export AWS_ACCESS_KEY_ID=${AWS_S3_ACCESS_KEY_FOR_PYTORCH_BINARY_UPLOAD}
            export AWS_SECRET_ACCESS_KEY=${AWS_S3_ACCESS_SECRET_FOR_PYTORCH_BINARY_UPLOAD}
            set +x
            echo "AWS KEY: ${AWS_ACCESS_KEY_ID}"
            echo "AWS SECRET: ${AWS_SECRET_ACCESS_KEY}"            
            aws s3 cp ${FILE_NAME} s3://ossci-ios-build/ --acl public-read

            # store the artifact
            mkdir -p ${WORKSPACE}/artifact/ios
            cp ${FILE_NAME} ${WORKSPACE}/artifact/ios

            
      - store_artifacts:
          path: ~/workspace/artifact
          destination: ${FILE_NAME}

##############################################################################
##############################################################################
# Workflows
##############################################################################
##############################################################################

# PR jobs pr builds
workflows:
  build:
    jobs:
      - setup
      # Pytorch iOS builds
      - pytorch_ios_build:
          name: pytorch_ios_10_2_1_x86_64_build
          context: org-member
          build_environment: "pytorch-ios-10.2.1-x86_64-build"
          ios_platform: "SIMULATOR"
          ios_arch: "x86_64"
          use_nnpack: "OFF"
          requires: 
            - setup
      - pytorch_ios_build:
          name: pytorch_ios_10_2_1_arm64_build
          context: org-member
          build_environment: "pytorch-ios-10.2.1-arm64-build"
          ios_arch: "arm64"
          ios_platform: "OS"
          requires: 
            - setup
      
